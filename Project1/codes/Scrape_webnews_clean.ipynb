{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#originated from NLPSA_scrape_clean.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yf_news_scraper(yf_link, name, Num_scroll = 20):\n",
    "    caps = DesiredCapabilities().CHROME\n",
    "    caps[\"pageLoadStrategy\"] = \"eager\"\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install(), desired_capabilities=caps)\n",
    "    driver.get(yf_link)\n",
    "    for _ in range(Num_scroll):\n",
    "        driver.execute_script(\"window.scrollTo(0,1000000)\")\n",
    "        time.sleep(2.5)\n",
    "    data_raw = driver.find_elements_by_xpath(\"//div[@data-test-locator='mega']//a\")\n",
    "    print('No. of news:',len(data_raw))\n",
    "    data_raw = driver.find_elements_by_xpath(\"//div[@data-test-locator='mega']//a\")\n",
    "    data_raw_topic = [i.text for i in data_raw]\n",
    "#    print(data_raw_topic)\n",
    "    urlss = [i.get_attribute('href') for i in data_raw]\n",
    "    for url in urlss:\n",
    "        if 'finance.yahoo' not in url:\n",
    "            dropid = urlss.index(url)\n",
    "            data_raw_topic.pop(dropid)\n",
    "            urlss.pop(dropid)\n",
    "\n",
    "    data_raw_date = []\n",
    "    data_raw_content = []\n",
    "    print('No. of filtered news:',len(data_raw))\n",
    "    urls = urlss[:]\n",
    "    print('start scraping')\n",
    "    for url in urls:\n",
    "        print(f'loading {url}')\n",
    "        driver.get(url)\n",
    "        print('finish loading pages')\n",
    "        data_raw_date.append(driver.find_element_by_tag_name('time').get_attribute('datetime'))\n",
    "        raw_p_content = driver.find_elements_by_xpath(\"//div[@class='caas-body']//p\")\n",
    "        print('finish finding date and content elements')\n",
    "\n",
    "        if len(raw_p_content) < 3:\n",
    "            try:\n",
    "                cr_link = driver.find_element_by_link_text('Continue reading').get_attribute('href')\n",
    "                data_raw_content.append(cr_link)\n",
    "                continue\n",
    "            except:\n",
    "                pass\n",
    "        content = ' '.join([i.text for i in raw_p_content])\n",
    "        data_raw_content.append(content)\n",
    "    print('Done scripting')\n",
    "\n",
    "    print(\"Shape:\",list(map(len, [data_raw_topic, data_raw_date, data_raw_content])))\n",
    "\n",
    "    _ = [print((i,j,k[:150]+'...'),\"\\n\") for i,j,k in zip(data_raw_date,data_raw_topic,data_raw_content)]\n",
    "    driver.quit()\n",
    "    print('quit driver.')\n",
    "    df = pd.DataFrame({'Date' : data_raw_date, 'topic': data_raw_topic, 'content': data_raw_content, 'url':urlss})\n",
    "    df.to_csv(f'raw_{name}.csv', index=False, header=True)"
   ]
  }
 ]
}