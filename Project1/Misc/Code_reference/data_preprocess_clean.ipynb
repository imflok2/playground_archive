{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd03410afedb74081d81603511028deadddc25ba0f01c14e0cb891e2c2473f81884",
   "display_name": "Python 3.8.8 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "3410afedb74081d81603511028deadddc25ba0f01c14e0cb891e2c2473f81884"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                       Date  \\\n",
       "0  2021-06-05T11:30:00.000Z   \n",
       "1  2021-06-04T20:05:19.000Z   \n",
       "2  2021-06-04T18:11:43.000Z   \n",
       "3  2021-06-04T15:03:06.000Z   \n",
       "4  2021-06-04T14:44:35.000Z   \n",
       "\n",
       "                                               topic  \\\n",
       "0             3 Meme Stocks That Could Make You Rich   \n",
       "1  Activists Launch Campaign Against Palantir's N...   \n",
       "2  Palantir Backs Multiple SPACs Endeavors in Dig...   \n",
       "3  UK healthcare app built by Iranian refugee set...   \n",
       "4  Continue to Exercise Caution When it Comes to ...   \n",
       "\n",
       "                                             content  \n",
       "0  https://www.fool.com/investing/2021/06/05/3-me...  \n",
       "1  A self-touted tech-justice non-profit, Foxglov...  \n",
       "2  Data analytics software developer Palantir Tec...  \n",
       "3  A British-Iranian entrepreneur who came to the...  \n",
       "4  Did Palantir (NYSE:PLTR) stock find its floor ...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>topic</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-06-05T11:30:00.000Z</td>\n      <td>3 Meme Stocks That Could Make You Rich</td>\n      <td>https://www.fool.com/investing/2021/06/05/3-me...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021-06-04T20:05:19.000Z</td>\n      <td>Activists Launch Campaign Against Palantir's N...</td>\n      <td>A self-touted tech-justice non-profit, Foxglov...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2021-06-04T18:11:43.000Z</td>\n      <td>Palantir Backs Multiple SPACs Endeavors in Dig...</td>\n      <td>Data analytics software developer Palantir Tec...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2021-06-04T15:03:06.000Z</td>\n      <td>UK healthcare app built by Iranian refugee set...</td>\n      <td>A British-Iranian entrepreneur who came to the...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2021-06-04T14:44:35.000Z</td>\n      <td>Continue to Exercise Caution When it Comes to ...</td>\n      <td>Did Palantir (NYSE:PLTR) stock find its floor ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "PLTR_raw = pd.read_csv('raw_PLTR.csv')\n",
    "PLTR_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLTR_raw['Date'] = PLTR_raw['Date'].apply(lambda x :datetime.datetime.strptime(x[:-8],r\"%Y-%m-%dT%H:%M\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 172 entries, 0 to 171\nData columns (total 3 columns):\n #   Column   Non-Null Count  Dtype         \n---  ------   --------------  -----         \n 0   Date     172 non-null    datetime64[ns]\n 1   topic    172 non-null    object        \n 2   content  172 non-null    object        \ndtypes: datetime64[ns](1), object(2)\nmemory usage: 4.2+ KB\n"
     ]
    }
   ],
   "source": [
    "PLTR_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 172 entries, 2021-06-05 11:30:00 to 2021-03-04 13:56:00\nData columns (total 2 columns):\n #   Column   Non-Null Count  Dtype \n---  ------   --------------  ----- \n 0   topic    172 non-null    object\n 1   content  172 non-null    object\ndtypes: object(2)\nmemory usage: 4.0+ KB\n"
     ]
    }
   ],
   "source": [
    "PLTR_raw.set_index('Date',inplace=True)\n",
    "PLTR_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLTR_raw.index = PLTR_raw.index.ceil('H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2021-05-11 12:00:00    4\n",
       "2021-05-11 21:00:00    3\n",
       "2021-05-11 16:00:00    3\n",
       "2021-05-11 11:00:00    3\n",
       "2021-05-11 15:00:00    3\n",
       "2021-05-11 13:00:00    3\n",
       "2021-06-04 21:00:00    2\n",
       "2021-04-08 14:00:00    2\n",
       "2021-05-21 18:00:00    2\n",
       "2021-05-24 11:00:00    2\n",
       "2021-04-05 15:00:00    2\n",
       "2021-04-29 22:00:00    2\n",
       "2021-05-11 18:00:00    2\n",
       "2021-05-12 13:00:00    2\n",
       "2021-05-06 11:00:00    2\n",
       "2021-03-25 15:00:00    2\n",
       "2021-05-26 19:00:00    2\n",
       "Name: Date, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "a = PLTR_raw.index.value_counts()\n",
    "a[a.values>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                                 topic  \\\n",
       "Date                                                                     \n",
       "2021-03-04 14:00:00  Palantir Rises on Latest Ark Investment Stock ...   \n",
       "2021-03-05 10:00:00  Jim Cramer on Jobs Report, Broadcom, Costco, P...   \n",
       "\n",
       "                                                               content  \n",
       "Date                                                                    \n",
       "2021-03-04 14:00:00  https://www.thestreet.com/investing/palantir-r...  \n",
       "2021-03-05 10:00:00  https://www.thestreet.com/jim-cramer/stock-mar...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic</th>\n      <th>content</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2021-03-04 14:00:00</th>\n      <td>Palantir Rises on Latest Ark Investment Stock ...</td>\n      <td>https://www.thestreet.com/investing/palantir-r...</td>\n    </tr>\n    <tr>\n      <th>2021-03-05 10:00:00</th>\n      <td>Jim Cramer on Jobs Report, Broadcom, Costco, P...</td>\n      <td>https://www.thestreet.com/jim-cramer/stock-mar...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "#process words columns\n",
    "for col in PLTR_raw.columns:\n",
    "    PLTR_raw[col] = PLTR_raw[col] + ' '\n",
    "PLTR_raw = PLTR_raw.groupby(level=0).sum()\n",
    "PLTR_raw.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_list_extended.txt','r') as f:\n",
    "    lmdict = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "special_words = {\n",
    "    'crushes': 10,\n",
    "    'beats': 5,\n",
    "    'misses': -5,\n",
    "    'trouble': -10,\n",
    "    'falls': -10,\n",
    "    'exploded' : 4,\n",
    "    'able':2,\n",
    "}\n",
    "\n",
    "for i in lmdict[\"Negative\"]:\n",
    "  special_words[i]=-5\n",
    "\n",
    "for i in lmdict[\"Positive\"]:\n",
    "  special_words[i]=5\n",
    "\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "vader.lexicon.update(special_words)\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for headline in news['title']:\n",
    "#   pol_score = vader.polarity_scores(headline)\n",
    "#   pol_score['headline'] = headline\n",
    "#   results.append(pol_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 148 entries, 2021-03-04 14:00:00 to 2021-06-05 12:00:00\nData columns (total 2 columns):\n #   Column   Non-Null Count  Dtype \n---  ------   --------------  ----- \n 0   topic    148 non-null    object\n 1   content  148 non-null    object\ndtypes: object(2)\nmemory usage: 3.5+ KB\n"
     ]
    }
   ],
   "source": [
    "def log_pos(x):\n",
    "    s = vader.polarity_scores(x)\n",
    "    return np.log((1 + s['pos'])/(1+s['neg']))\n",
    "PLTR_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLTR_raw['topic_comp'] = PLTR_raw['topic'].apply(lambda x: vader.polarity_scores(x)['compound'])\n",
    "PLTR_raw['content_comp'] = PLTR_raw['content'].apply(lambda x: vader.polarity_scores(x)['compound'])\n",
    "PLTR_raw['topic_logpos'] = PLTR_raw['topic'].apply(log_pos)\n",
    "PLTR_raw['content_logpos'] = PLTR_raw['content'].apply(log_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================# Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "PLTR_price = yf.download(tickers='PLTR', period='6mo', interval=\"1h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "PLTR_price.index = PLTR_price.index.tz_convert(None)\n",
    "PLTR_price.index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================# Produce df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                     topic_comp  content_comp  topic_logpos  content_logpos  \\\n",
       "2020-12-09 14:30:00         0.0           0.0           0.0             0.0   \n",
       "2020-12-09 15:30:00         0.0           0.0           0.0             0.0   \n",
       "2020-12-09 16:30:00         0.0           0.0           0.0             0.0   \n",
       "2020-12-09 17:30:00         0.0           0.0           0.0             0.0   \n",
       "2020-12-09 18:30:00         0.0           0.0           0.0             0.0   \n",
       "\n",
       "                          Open       High        Low      Close  Adj Close  \\\n",
       "2020-12-09 14:30:00  28.680000  28.850000  26.709999  27.420000  27.420000   \n",
       "2020-12-09 15:30:00  27.424999  27.900000  27.190001  27.410000  27.410000   \n",
       "2020-12-09 16:30:00  27.410000  27.480000  26.950001  27.089899  27.089899   \n",
       "2020-12-09 17:30:00  27.084999  27.200001  26.040001  26.113400  26.113400   \n",
       "2020-12-09 18:30:00  26.139999  26.639999  25.330000  26.594999  26.594999   \n",
       "\n",
       "                         Volume  \n",
       "2020-12-09 14:30:00  25848861.0  \n",
       "2020-12-09 15:30:00   7646543.0  \n",
       "2020-12-09 16:30:00   7067566.0  \n",
       "2020-12-09 17:30:00  10450521.0  \n",
       "2020-12-09 18:30:00  15899190.0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic_comp</th>\n      <th>content_comp</th>\n      <th>topic_logpos</th>\n      <th>content_logpos</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Adj Close</th>\n      <th>Volume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-12-09 14:30:00</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>28.680000</td>\n      <td>28.850000</td>\n      <td>26.709999</td>\n      <td>27.420000</td>\n      <td>27.420000</td>\n      <td>25848861.0</td>\n    </tr>\n    <tr>\n      <th>2020-12-09 15:30:00</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>27.424999</td>\n      <td>27.900000</td>\n      <td>27.190001</td>\n      <td>27.410000</td>\n      <td>27.410000</td>\n      <td>7646543.0</td>\n    </tr>\n    <tr>\n      <th>2020-12-09 16:30:00</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>27.410000</td>\n      <td>27.480000</td>\n      <td>26.950001</td>\n      <td>27.089899</td>\n      <td>27.089899</td>\n      <td>7067566.0</td>\n    </tr>\n    <tr>\n      <th>2020-12-09 17:30:00</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>27.084999</td>\n      <td>27.200001</td>\n      <td>26.040001</td>\n      <td>26.113400</td>\n      <td>26.113400</td>\n      <td>10450521.0</td>\n    </tr>\n    <tr>\n      <th>2020-12-09 18:30:00</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>26.139999</td>\n      <td>26.639999</td>\n      <td>25.330000</td>\n      <td>26.594999</td>\n      <td>26.594999</td>\n      <td>15899190.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "df = pd.concat([PLTR_raw.drop(['topic','content'],axis=1),PLTR_price], axis=1)\n",
    "df.fillna(0,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 617 entries, 2021-03-04 14:00:00 to 2021-06-08 19:30:00\nData columns (total 10 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   topic_comp      617 non-null    float64\n 1   content_comp    617 non-null    float64\n 2   topic_logpos    617 non-null    float64\n 3   content_logpos  617 non-null    float64\n 4   Open            617 non-null    float64\n 5   High            617 non-null    float64\n 6   Low             617 non-null    float64\n 7   Close           617 non-null    float64\n 8   Adj Close       617 non-null    float64\n 9   Volume          617 non-null    float64\ndtypes: float64(10)\nmemory usage: 53.0 KB\n"
     ]
    }
   ],
   "source": [
    "df_slim = df[df.index>datetime.datetime(2021,3,4,10)]\n",
    "df_slim.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.0    925\n1.0     88\nName: topic_comp, dtype: int64, 0.0    931\n1.0     82\nName: content_comp, dtype: int64]\n"
     ]
    }
   ],
   "source": [
    "print(list(map(lambda x : df[x].mask(df[x] != 0, 1).value_counts(), ['topic_comp','content_comp'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = df_slim['Adj Close']\n",
    "X = df_slim.drop(['Adj Close','Close'],axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(X_train[['Open', 'High', 'Low', 'Volume']])\n",
    "X_train_scaled = pd.concat([X_train.iloc[:,:4], pd.DataFrame(X_train_scaled,index=X_train.index)], axis=1)\n",
    "\n",
    "sc_predict = StandardScaler()\n",
    "X_test_scaled = sc_predict.fit_transform(X_test[['Open','High', 'Low', 'Volume']])\n",
    "X_tset_scaled = pd.concat([X_test.iloc[:,:4], pd.DataFrame(X_test_scaled,index=X_test.index)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "X_train_scaled.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "train_scaled = X_train_scaled.join(y_train)\n",
    "train_scaled.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled = train_scaled.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_scaled shape: (555, 9)\nX_train shape == (507, 48, 9).\ny_train shape == (507, 1).\n"
     ]
    }
   ],
   "source": [
    "print('train_scaled shape:',train_scaled.shape)\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "n_future = 1   # Number of days (*12 for hours) we want to predict into the future\n",
    "n_past = 4*12   # Number of past days (*12 for hours) we want to use to predict the future\n",
    "\n",
    "for i in range(n_past, len(train_scaled) - n_future +1):\n",
    "    X_train.append(train_scaled[i - n_past:i, 0: train_scaled.shape[1]])\n",
    "    y_train.append(train_scaled[i + n_future - 1:i + n_future, -1])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "print('X_train shape == {}.'.format(X_train.shape))\n",
    "print('y_train shape == {}.'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_x_train, All_y_train = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Neural Network based on LSTM\n",
    "model = Sequential()\n",
    "\n",
    "# Adding 1st LSTM layer\n",
    "model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "\n",
    "# Adding 2nd LSTM layer\n",
    "model.add(LSTM(units=10, return_sequences=False))\n",
    "\n",
    "# Adding Dropout\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(units=1, activation='linear'))\n",
    "#model.add(Dense(trainY.shape[1]))\n",
    "\n",
    "# Compiling the Neural Network\n",
    "model.compile(optimizer = Adam(learning_rate=0.01), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/300\n",
      "14/14 [==============================] - 12s 512ms/step - loss: 358.8819 - val_loss: 200.3942\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 200.39421, saving model to weights.h5\n",
      "Epoch 2/300\n",
      "14/14 [==============================] - 1s 54ms/step - loss: 248.9069 - val_loss: 166.5180\n",
      "\n",
      "Epoch 00002: val_loss improved from 200.39421 to 166.51797, saving model to weights.h5\n",
      "Epoch 3/300\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 216.9517 - val_loss: 139.6680\n",
      "\n",
      "Epoch 00003: val_loss improved from 166.51797 to 139.66795, saving model to weights.h5\n",
      "Epoch 4/300\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 178.9663 - val_loss: 121.8657\n",
      "\n",
      "Epoch 00004: val_loss improved from 139.66795 to 121.86569, saving model to weights.h5\n",
      "Epoch 5/300\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 169.2937 - val_loss: 108.7850\n",
      "\n",
      "Epoch 00005: val_loss improved from 121.86569 to 108.78497, saving model to weights.h5\n",
      "Epoch 6/300\n",
      "14/14 [==============================] - 1s 41ms/step - loss: 141.6425 - val_loss: 99.7719\n",
      "\n",
      "Epoch 00006: val_loss improved from 108.78497 to 99.77193, saving model to weights.h5\n",
      "Epoch 7/300\n",
      "14/14 [==============================] - 1s 39ms/step - loss: 133.2956 - val_loss: 93.8947\n",
      "\n",
      "Epoch 00007: val_loss improved from 99.77193 to 93.89474, saving model to weights.h5\n",
      "Epoch 8/300\n",
      "14/14 [==============================] - 1s 49ms/step - loss: 125.7416 - val_loss: 90.2398\n",
      "\n",
      "Epoch 00008: val_loss improved from 93.89474 to 90.23981, saving model to weights.h5\n",
      "Epoch 9/300\n",
      "14/14 [==============================] - 1s 51ms/step - loss: 118.8388 - val_loss: 88.2791\n",
      "\n",
      "Epoch 00009: val_loss improved from 90.23981 to 88.27910, saving model to weights.h5\n",
      "Epoch 10/300\n",
      "14/14 [==============================] - 1s 40ms/step - loss: 105.4153 - val_loss: 87.3973\n",
      "\n",
      "Epoch 00010: val_loss improved from 88.27910 to 87.39735, saving model to weights.h5\n",
      "Epoch 11/300\n",
      "14/14 [==============================] - 1s 40ms/step - loss: 109.1068 - val_loss: 87.2974\n",
      "\n",
      "Epoch 00011: val_loss improved from 87.39735 to 87.29745, saving model to weights.h5\n",
      "Epoch 12/300\n",
      "14/14 [==============================] - 1s 37ms/step - loss: 106.6055 - val_loss: 87.5379\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 87.29745\n",
      "Epoch 13/300\n",
      "14/14 [==============================] - 1s 39ms/step - loss: 107.4195 - val_loss: 87.8683\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 87.29745\n",
      "Epoch 14/300\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 110.1340 - val_loss: 88.2423\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 87.29745\n",
      "Epoch 15/300\n",
      "14/14 [==============================] - 1s 38ms/step - loss: 107.6344 - val_loss: 88.5790\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 87.29745\n",
      "Epoch 16/300\n",
      "14/14 [==============================] - 1s 39ms/step - loss: 95.4549 - val_loss: 89.2214\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 87.29745\n",
      "Epoch 17/300\n",
      "14/14 [==============================] - 1s 41ms/step - loss: 96.4137 - val_loss: 89.5010\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 87.29745\n",
      "Epoch 18/300\n",
      "14/14 [==============================] - 1s 38ms/step - loss: 106.9564 - val_loss: 89.7039\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 87.29745\n",
      "Epoch 19/300\n",
      "14/14 [==============================] - 1s 37ms/step - loss: 109.3782 - val_loss: 89.7691\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 87.29745\n",
      "Epoch 20/300\n",
      "14/14 [==============================] - 0s 36ms/step - loss: 101.3834 - val_loss: 90.0418\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 87.29745\n",
      "Epoch 21/300\n",
      "14/14 [==============================] - 1s 37ms/step - loss: 95.7145 - val_loss: 90.1066\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 87.29745\n",
      "Epoch 00021: early stopping\n",
      "Wall time: 26.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=10, verbose=1)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)\n",
    "mcp = ModelCheckpoint(filepath='weights.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "tb = TensorBoard('logs')\n",
    "\n",
    "history = model.fit(X_train, y_train, shuffle=True, epochs=300, callbacks=[es, rlr, mcp, tb], validation_split=0.2, verbose=1, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}